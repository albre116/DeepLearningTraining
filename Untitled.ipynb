{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearningTraining\n",
    "Training Materials for Deep Learning on Google Cloud with Fast AI ([fastai course](http://course.fast.ai/)).\n",
    "\n",
    "We port over the course content to be run on google cloud instead of AWS.  This will take about 1/2 a day a week for a 2 month period to complete.\n",
    "\n",
    "## VM machine and GPU and setup\n",
    "\n",
    "* Go to the google cloud [console](https://console.cloud.google.com) and log into the platform \n",
    "* Under compute engine => VM instances\n",
    "* Under the compute Engine dashboard select create instance\n",
    "    * Review which regions GPUs are available in [GPU Available Regions](https://cloud.google.com/compute/docs/gpus/) \n",
    "    * Pick a VM from that zone (i.e. us-east1-c is suitable)\n",
    "    * Select a 1 vCPU system and click customize to GPU tab below\n",
    "    * Select NVIDIA Tesla K80 (which is really a K40 board (or 1/2 a K80) if you do 1 GPU... talk about bad advertising)\n",
    "    * Change the OS and storage drive to be Ubuntu 16.04 LTS and go for a 100GB drive (instead of default 10GB)\n",
    "* Launch the image with Create\n",
    "    * Google Cloud has a nice SSH client you can just use in the browser instead of putty\n",
    "        * terminal in as root for all commands below by running ``` sudo su ```\n",
    "    * As a note, you will be using a corporate account which already has added firewall policies to unblock ports 80,443,8888 so you will not need to do that\n",
    "* Clone this repository to gain access to the build scripts\n",
    "    * Following the documentation for [Creating a GPU instance](https://cloud.google.com/compute/docs/gpus/add-gpus#create-new-gpu-instance)\n",
    "        * Run the cuda_bare_metal.sh install file found in the setup folder ``` sh cuda_bare_metal.sh ```\n",
    "        * Verify your install by running at the command line ``` nvidia-smi ```\n",
    "## NVIDIA Docker Install\n",
    "We will use the docker engine to connect to the underlying CUDA drivers, but this requires and additional driver set that makese the connection between docker and the bare metal CUDA drivers.\n",
    "![nvidia-gpu-docker](https://cloud.githubusercontent.com/assets/3028125/12213714/5b208976-b632-11e5-8406-38d379ec46aa.png)\n",
    "\n",
    "* run the file getdrivers_dockerce.sh by ```sh getdrivers_dockerce.sh```\n",
    "* test the install by running ``` docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
